{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing the News Headlines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geonamescache\n",
    "from collections import Counter\n",
    "import unidecode\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the text file and make a dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines = pd.read_csv('discovering-disease-outbreaks-base/data/headlines.txt',\n",
    "                        header=None, \n",
    "                        delimiter='\\n',\n",
    "                        names=['Headlines'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headlines</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>More Zika patients reported in Calumpang</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>Laventille authorities confirmed the spread of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434</th>\n",
       "      <td>Case of Mad Cow Disease Reported in Hilden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>Zika spreads to Daytona Beach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>Hospitals in Hanoi fill up with Zika patients</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Headlines\n",
       "368           More Zika patients reported in Calumpang\n",
       "362  Laventille authorities confirmed the spread of...\n",
       "434         Case of Mad Cow Disease Reported in Hilden\n",
       "218                      Zika spreads to Daytona Beach\n",
       "72       Hospitals in Hanoi fill up with Zika patients"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headlines.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of headlines: 650\n",
      "Max lenght of headlines: 87\n",
      "Min lenght of headlines: 16\n",
      "Average lenght of headlines: 40.77\n"
     ]
    }
   ],
   "source": [
    "# Check some informations about headlines\n",
    "print(\"Number of headlines: {}\".format(headlines.shape[0]))\n",
    "print(\"Max lenght of headlines: {}\".format(max([len(each[0]) for each in headlines.values])))\n",
    "print(\"Min lenght of headlines: {}\".format(min([len(each[0]) for each in headlines.values])))\n",
    "print(\"Average lenght of headlines: {:.2f}\".format(np.mean([len(each[0]) for each in headlines.values])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Countries and Cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init geonamecashe instance\n",
    "gc = geonamescache.GeonamesCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the name of all the cities into a list\n",
    "cities = [city['name'] for city in gc.get_cities().values()]\n",
    "# Extract all the country names into a list\n",
    "countries = [country['name'] for country in gc.get_countries().values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of cities: 24336\n",
      "Total number of countries: 252\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of cities: {}\".format(len(cities)))\n",
    "print(\"Total number of countries: {}\".format(len(countries)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Springfield', 8),\n",
       " ('San Pedro', 7),\n",
       " ('Richmond', 7),\n",
       " ('San Fernando', 7),\n",
       " ('Mercedes', 6),\n",
       " ('La Paz', 6),\n",
       " ('Victoria', 6),\n",
       " ('San Francisco', 6),\n",
       " ('Auburn', 6),\n",
       " ('Santa Cruz', 6)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find duplicate\n",
    "city_counter = Counter(cities)\n",
    "city_counter.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove accent marks\n",
    "country_accent_mapping = {\n",
    "    unidecode.unidecode(country): country for country in countries}\n",
    "\n",
    "city_accent_mapping = {\n",
    "    unidecode.unidecode(city): city for city in cities\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the dataframe from accent marks\n",
    "headlines_clean = pd.DataFrame(\n",
    "    {\n",
    "        \"Headlines\":[unidecode.unidecode(headline[0]) for headline in headlines.values]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned cities and countries into a list\n",
    "clean_cities = list(city_accent_mapping.keys())\n",
    "clean_countries = set(country_accent_mapping.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the city and country names\n",
    "clean_cities = sorted(clean_cities, key=lambda x: len(x), reverse=True)\n",
    "clean_countries = sorted(clean_countries, key=lambda x:len(x), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_regex = r'\\b|\\b'.join(clean_cities)\n",
    "country_regex = r'\\b|\\b'.join(clean_countries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_country_and_city(headline):\n",
    "    city = re.search(city_regex, headline)\n",
    "    country = re.search(country_regex, headline)\n",
    "    cities = None if not city else city.group(0)\n",
    "    countries = None if not country else country.group(0)\n",
    "    return dict(headline=headline, countries=countries, cities=cities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines_extracted_cities_countries = [\n",
    "    find_country_and_city(headline[0]) for headline in headlines.values\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.DataFrame(headlines_extracted_cities_countries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save files for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_file = \"headlines_with_city_country.json\"\n",
    "with open(save_file, 'w') as f:\n",
    "    f.write(json.dumps(headlines_extracted_cities_countries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"city_accent_mapping.json\", 'w') as f:\n",
    "    f.write(json.dumps(city_accent_mapping))\n",
    "    \n",
    "with open(\"country_accent_mapping.json\", 'w') as f:\n",
    "    f.write(json.dumps(country_accent_mapping))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_country_and_city_v2(headline):\n",
    "    city_match = re.search(city_regex, headline)\n",
    "    country_match = re.search(country_regex, headline)\n",
    "    city = None if not city_match else city_match.group(0)\n",
    "    country = None if not country_match else country_match.group(0)\n",
    "\n",
    "    possible_city = sorted([each for each in gc.get_cities_by_name(city)], \n",
    "                                 key=lambda x:list(x.values())[0]['population'], \n",
    "                                 reverse=True)\n",
    "    \n",
    "    \n",
    "    if len(possible_city) > 0:\n",
    "        countrycode = list(possible_city[0].values())[0].get('countrycode')\n",
    "        lat = list(possible_city[0].values())[0].get('latitude')\n",
    "        lon = list(possible_city[0].values())[0].get('longitude')\n",
    "        id = list(possible_city[0].values())[0].get('geonameid')  \n",
    "        country=gc.get_countries().get(countrycode).get('name')\n",
    "    \n",
    "    else:\n",
    "        lat = None\n",
    "        lon = None\n",
    "        id = None\n",
    "        \n",
    "    return dict(headline=headline, country=country, city=city, id=str(id), latitude=lat, longitude=lon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines_extracted_cities_countries_v2 = [\n",
    "    find_country_and_city_v2(headline[0]) for headline in headlines.values\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "headlines_v2 = pd.DataFrame(headlines_extracted_cities_countries_v2).sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
